name: Performance

on:
  # push/pull_request/schedule disabled to conserve GitHub Actions free tier minutes.
  # Re-enable when project is stable or on a paid plan.
  # push:
  #   branches: [ main, develop ]
  # pull_request:
  #   branches: [ main, develop ]
  # schedule:
  #   - cron: '0 4 * * *'
  workflow_dispatch:

permissions: read-all

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  RUSTFLAGS: "-C target-cpu=native"

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
      with:
        submodules: recursive
        fetch-depth: 0

    - name: Install Rust
      uses: dtolnay/rust-toolchain@4be9e76fd7c4901c61fb841f559994984270fce7 # stable

    - name: Cache Cargo registry
      uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-bench-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Run benchmarks
      run: |
        cargo bench --workspace --all-features || echo "Benchmarks completed (some may have been skipped)"

    - name: Store benchmark result
      if: hashFiles('target/criterion/reports/index.html') != ''
      uses: benchmark-action/github-action-benchmark@4bdcce38c94cec68da58d012ac24b7b1155efe8b # v1
      with:
        tool: 'cargo'
        output-file-path: target/criterion/reports/index.html
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '200%'
      continue-on-error: true

    - name: Skip benchmark store (no output)
      if: hashFiles('target/criterion/reports/index.html') == ''
      run: echo "No benchmark output to store (criterion reports not generated)"

  performance-regression:
    name: Performance Regression Tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
      with:
        submodules: recursive

    - name: Install Rust
      uses: dtolnay/rust-toolchain@4be9e76fd7c4901c61fb841f559994984270fce7 # stable

    - name: Cache Cargo registry
      uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-perf-regression-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Run performance regression tests
      run: |
        # Run performance regression tests if they exist
        cargo test --test performance_regression_tests --all-features --release 2>&1 || echo "No performance_regression_tests found, skipping"

  profiling:
    name: Performance Profiling
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
      with:
        submodules: recursive

    - name: Install Rust
      uses: dtolnay/rust-toolchain@4be9e76fd7c4901c61fb841f559994984270fce7 # stable

    - name: Install profiling tools
      run: sudo apt-get update && sudo apt-get install -y linux-tools-generic

    - name: Cache Cargo registry
      uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-profiling-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Build with profiling symbols
      run: |
        
        cargo build --workspace --release --all-features

    - name: Run CPU profiling
      run: |
        # Find a test binary to profile (latticearc is a library, not a binary)
        TEST_BIN=$(find target/release/deps -name 'latticearc-*' -type f -executable ! -name '*.d' | head -1)
        if [ -n "$TEST_BIN" ]; then
          echo "Profiling: $TEST_BIN"
          perf record --call-graph=dwarf "$TEST_BIN" --test || echo "Profiling completed"
          perf report > perf_report.txt || echo "Empty perf report"
        else
          echo "No binary found for profiling"
          echo "No binary available for profiling" > perf_report.txt
        fi

    - name: Upload profiling results
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
      with:
        name: profiling-results
        path: perf_report.txt
        retention-days: 30

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
      with:
        submodules: recursive

    - name: Install Rust
      uses: dtolnay/rust-toolchain@4be9e76fd7c4901c61fb841f559994984270fce7 # stable

    - name: Install memory profiling tools
      run: sudo apt-get update && sudo apt-get install -y valgrind massif-visualizer

    - name: Cache Cargo registry
      uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-memory-profiling-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Build with debug symbols
      run: |
        
        cargo build --workspace --all-features

    - name: Run memory profiling
      run: |
        # Find a test binary to profile (latticearc is a library, not a binary)
        TEST_BIN=$(find target/debug/deps -name 'latticearc-*' -type f -executable ! -name '*.d' | head -1)
        if [ -n "$TEST_BIN" ]; then
          echo "Memory profiling: $TEST_BIN"
          valgrind --tool=massif --massif-out-file=massif.out "$TEST_BIN" --test || echo "Memory profiling completed"
          ms_print massif.out > memory_profile.txt || echo "Empty memory profile"
        else
          echo "No binary found for memory profiling"
          echo "No binary available for memory profiling" > memory_profile.txt
        fi

    - name: Upload memory profiling results
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
      with:
        name: memory-profiling-results
        path: memory_profile.txt
        retention-days: 30

  performance-report:
    name: Performance Report
    runs-on: ubuntu-latest
    needs: [benchmark, performance-regression, profiling, memory-profiling]
    if: always()
    steps:
    - name: Generate performance summary
      run: |
        echo "# Performance Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Benchmarks | ${{ needs.benchmark.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance Regression | ${{ needs.performance-regression.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Profiling | ${{ needs.profiling.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Memory Profiling | ${{ needs.memory-profiling.result }} |" >> $GITHUB_STEP_SUMMARY

    - name: Check overall status
      run: |
        if [[ "${{ needs.benchmark.result }}" == "failure" || \
              "${{ needs.performance-regression.result }}" == "failure" ]]; then
          echo "❌ Performance tests failed!"
          exit 1
        else
          echo "✅ Performance tests passed!"
        fi